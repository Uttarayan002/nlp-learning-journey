# -*- coding: utf-8 -*-
"""How Machines Read Text

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zbn7p-kRbBbbLQA_jWz4qbdTlunmffak

# Tokenization → Stopword Removal → Lemmatization
**Think of it as**: Splitting words → Removing noise → Reducing words to their base form
"""

!pip install nltk

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download resources
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
nltk.download('punkt_tab')

text = "Cats running faster than the dogs in the yard."
tokens = word_tokenize(text)

print(tokens)  #breaking down a sentence into individual units, usually words or subwords called tokens.

stop_word =set(stopwords.words('english'))  #don’t carry useful meaning for modeling.
filtered = [w for w in tokens if w.lower() not in stop_word]

print(stop_word)

# Lemmatization
lemmatizer = WordNetLemmatizer()   # reducing words to their base form (lemma) using linguistic knowledge.
lemmatized = [lemmatizer.lemmatize(word) for word in filtered]

print(lemmatized)

print("Original Tokens:", tokens)
print("Filtered Tokens:", filtered)
print("Lemmatized Tokens:", lemmatized)