# -*- coding: utf-8 -*-
"""Text Cleaning Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1slmdJnljj5OFVFicKoIgnpuCXnxWyI-9
"""

import nltk
import string
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

# Download resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

#initialize
stopw = set(stopwords.words('english'))
lemma = WordNetLemmatizer()
nlp = spacy.load('en_core_web_sm')

def clean_text(text):

  text = text.lower()

  text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])

  token = word_tokenize(text)

  token = [word for word in token if word not in stopw]

  token = [lemma.lemmatize(w) for w in token]

  return " ".join(token)

texts = " Data science is one of the hottest topic right now, along with LLM"
cleaning = clean_text(texts)

print(f"clean texts -> {cleaning}")

"""_______________________________________________________________________________________________________________________________________________________________
# Let's practise another sentence
"""

def clean_text(text):

  text = text.lower()

  text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])

  token = word_tokenize(text)

  token = [word for word in token if word not in stopw]

  token = [lemma.lemmatize(w) for w in token]

  return " ".join(token)

test_sent = "In 2024, Uttarayan explored NLP -  learning, failing & growing!"
cleaning = clean_text(test_sent)

print(f"clean texts -> {cleaning}")









